# Nháº­n Dáº¡ng Chá»¯ Sá»‘ MNIST vá»›i CNN

## ðŸŽ¯ MÃ´ táº£ dá»± Ã¡n

ÄÃ¢y lÃ  dá»± Ã¡n nháº­n dáº¡ng chá»¯ sá»‘ viáº¿t tay tá»« dataset MNIST sá»­ dá»¥ng Convolutional Neural Network (CNN). Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn trong khuÃ´n khá»• **Olympic AI - Buá»•i 3 - BÃ i táº­p E1** vá»›i má»¥c tiÃªu Ä‘áº¡t Ä‘Æ°á»£c **accuracy cao nháº¥t cÃ³ thá»ƒ** trÃªn dataset MNIST.

### ðŸ† **Káº¿t quáº£ Ä‘áº¡t Ä‘Æ°á»£c: 99.40% Validation Accuracy**

## ðŸŽ¯ Má»¥c tiÃªu

- **Má»¥c tiÃªu chÃ­nh**: LÃ m quen vá»›i CNN cÆ¡ báº£n vÃ  Ä‘áº¡t accuracy cao
- **Dataset**: Digit Recognizer (MNIST) - 70,000 áº£nh chá»¯ sá»‘ viáº¿t tay 28x28 pixel
- **Metric Ä‘Ã¡nh giÃ¡**: Accuracy 
- **Kiáº¿n trÃºc**: CNN vá»›i cÃ¡c layer Convâ€“ReLUâ€“Pool, BatchNorm, data augmentation
- **Target**: Äáº¡t >99% accuracy trÃªn validation set âœ…

## ðŸ§  CNN lÃ  gÃ¬ vÃ  táº¡i sao phÃ¹ há»£p vá»›i MNIST?

### Convolutional Neural Network (CNN)
CNN lÃ  má»™t loáº¡i neural network Ä‘áº·c biá»‡t hiá»‡u quáº£ cho cÃ¡c tÃ¡c vá»¥ xá»­ lÃ½ áº£nh:

1. **Convolutional Layer**: PhÃ¡t hiá»‡n cÃ¡c Ä‘áº·c trï¿½ng cá»¥c bá»™ (edges, patterns)
2. **Pooling Layer**: Giáº£m kÃ­ch thÆ°á»›c vÃ  tÄƒng tÃ­nh báº¥t biáº¿n
3. **Fully Connected Layer**: PhÃ¢n loáº¡i dá»±a trÃªn features Ä‘Ã£ trÃ­ch xuáº¥t

### Táº¡i sao CNN phÃ¹ há»£p vá»›i MNIST?
- **Spatial Information**: CNN giá»¯ Ä‘Æ°á»£c thÃ´ng tin khÃ´ng gian cá»§a pixel
- **Parameter Sharing**: Giáº£m sá»‘ parameters so vá»›i Dense layer
- **Translation Invariance**: Nháº­n dáº¡ng Ä‘Æ°á»£c chá»¯ sá»‘ á»Ÿ cÃ¡c vá»‹ trÃ­ khÃ¡c nhau
- **Feature Hierarchy**: Há»c Ä‘Æ°á»£c cÃ¡c pattern tá»« Ä‘Æ¡n giáº£n Ä‘áº¿n phá»©c táº¡p

## Äáº·c Ä‘iá»ƒm ká»¹ thuáº­t

### Kiáº¿n trÃºc mÃ´ hÃ¬nh

MÃ´ hÃ¬nh CNN Ä‘Æ°á»£c thiáº¿t káº¿ vá»›i:

1. **Data Augmentation Layer**:
   - RandomRotation (10%)
   - RandomZoom (10%) 
   - RandomTranslation (10%)

2. **3 Convolutional Blocks**:
   - **Block 1**: 2x Conv2D(32) + BatchNorm + MaxPool + Dropout(0.25)
   - **Block 2**: 2x Conv2D(64) + BatchNorm + MaxPool + Dropout(0.25)
   - **Block 3**: Conv2D(128) + BatchNorm + Dropout(0.25)

3. **Dense Layers**:
   - Flatten
   - Dense(512) + BatchNorm + Dropout(0.5)
   - Dense(256) + Dropout(0.5) 
   - Dense(10, softmax) - output layer

## ðŸ“Š PhÃ¢n tÃ­ch káº¿t quáº£ chi tiáº¿t

### ðŸŽ¯ **Táº¡i sao Ä‘áº¡t Ä‘Æ°á»£c 99.40% accuracy?**

1. **Kiáº¿n trÃºc CNN phÃ¹ há»£p**:
   - 3 Conv blocks vá»›i sá»‘ filters tÄƒng dáº§n (32â†’64â†’128)
   - MaxPooling giÃºp táº¡o translation invariance
   - BatchNorm giÃºp training á»•n Ä‘á»‹nh

2. **Data Augmentation hiá»‡u quáº£**:
   - RandomRotation: Nháº­n dáº¡ng chá»¯ sá»‘ bá»‹ xoay nháº¹
   - RandomZoom: ThÃ­ch á»©ng vá»›i kÃ­ch thÆ°á»›c khÃ¡c nhau  
   - RandomTranslation: Chá»¯ sá»‘ khÃ´ng cáº§n á»Ÿ chÃ­nh giá»¯a

3. **Regularization tá»‘t**:
   - Dropout ngÄƒn overfitting
   - Early Stopping dá»«ng Ä‘Ãºng lÃºc
   - Learning rate scheduling fine-tune cuá»‘i

4. **Preprocessing chuáº©n**:
   - Normalization [0,255] â†’ [0,1]
   - One-hot encoding cho multi-class
   - Train/validation split stratified

### ðŸ“ˆ **Biá»ƒu Ä‘á»“ káº¿t quáº£**

Project tá»± Ä‘á»™ng táº¡o ra 3 biá»ƒu Ä‘á»“ quan trá»ng:

1. **`training_history.png`**: 
   - Training vs Validation Accuracy/Loss theo epochs
   - Tháº¥y Ä‘Æ°á»£c overfitting hay underfitting
   - Xem hiá»‡u quáº£ cá»§a learning rate scheduling

2. **`confusion_matrix.png`**:
   - Ma tráº­n 10x10 cho digits 0-9
   - Xem model hay nháº§m láº«n giá»¯a sá»‘ nÃ o
   - ÄÆ°á»ng chÃ©o chÃ­nh â†’ predictions chÃ­nh xÃ¡c

3. **`predictions_sample.png`**:
   - 10 áº£nh test Ä‘áº§u tiÃªn vá»›i predictions
   - Kiá»ƒm tra trá»±c quan model cÃ³ hoáº¡t Ä‘á»™ng Ä‘Ãºng

### ðŸ” **PhÃ¢n tÃ­ch Confusion Matrix**

Tá»« káº¿t quáº£ thá»±c táº¿, model ráº¥t Ã­t nháº§m láº«n:
- **Digit 1**: 99% precision/recall (it nháº§m vá»›i 7)
- **Digit 8**: 99% precision/recall (it nháº§m vá»›i 0, 6)  
- **Digit 9**: 99% precision/recall (it nháº§m vá»›i 4, 7)

â†’ **Káº¿t luáº­n**: Model há»c Ä‘Æ°á»£c Ä‘áº·c trÆ°ng ráº¥t tá»‘t!

## ðŸ“¦ CÃ i Ä‘áº·t vÃ  cháº¡y

### ðŸ“‹ **YÃªu cáº§u há»‡ thá»‘ng**
- **Python**: 3.8+ (Recommended: 3.9-3.11)
- **RAM**: Tá»‘i thiá»ƒu 4GB (Recommended: 8GB+)
- **Storage**: ~500MB cho code + data + results
- **GPU**: KhÃ´ng báº¯t buá»™c nhÆ°ng khuyáº¿n nghá»‹ (giáº£m thá»i gian tá»« 15 phÃºt â†’ 3 phÃºt)

### 1. ðŸš€ **CÃ i Ä‘áº·t nhanh (Recommended)**

```bash
# BÆ°á»›c 1: Clone repository
git clone https://github.com/nguyentrungkiet/buoi3_e1.git
cd buoi3_e1

# BÆ°á»›c 2: Táº¡o virtual environment (Recommended)
python -m venv venv

# KÃ­ch hoáº¡t virtual environment
# Windows:
venv\Scripts\activate
# macOS/Linux:  
source venv/bin/activate

# BÆ°á»›c 3: CÃ i Ä‘áº·t dependencies
pip install -r requirements.txt

# BÆ°á»›c 4: Cháº¡y training
python resolve.py
```

### 2. ðŸ **CÃ i Ä‘áº·t tá»«ng bÆ°á»›c (Chi tiáº¿t)**

```bash
# Kiá»ƒm tra Python version
python --version  # Pháº£i >= 3.8

# Táº¡o thÆ° má»¥c project
mkdir mnist_cnn_project
cd mnist_cnn_project

# Clone code
git clone https://github.com/nguyentrungkiet/buoi3_e1.git .

# Kiá»ƒm tra cáº¥u trÃºc
ls -la  # Pháº£i tháº¥y resolve.py, requirements.txt, dataset/

# CÃ i Ä‘áº·t tá»«ng package (náº¿u pip install -r khÃ´ng work)
pip install tensorflow>=2.8.0
pip install pandas>=1.3.0  
pip install numpy>=1.21.0
pip install matplotlib>=3.4.0
pip install seaborn>=0.11.0
pip install scikit-learn>=1.0.0

# Kiá»ƒm tra installation
python -c "import tensorflow as tf; print(f'TensorFlow: {tf.__version__}')"
```

### 3. ðŸ“ **Kiá»ƒm tra dá»¯ liá»‡u**

```bash
# VÃ o thÆ° má»¥c dataset
cd dataset

# Kiá»ƒm tra files cÃ³ Ä‘á»§ khÃ´ng
ls -la
# Pháº£i tháº¥y:
# train.csv (73MB - 42,000 samples)
# test.csv (48MB - 28,000 samples)  
# sample_submission.csv (nhá» - format máº«u)

# Kiá»ƒm tra dá»¯ liá»‡u train
head -n 2 train.csv
# Output mong Ä‘á»£i:
# label,pixel0,pixel1,pixel2,...,pixel783
# 1,0,0,0,0,0,0,0,0,0,...
```

### 4. â–¶ï¸ **Cháº¡y training**

```bash
# Vá» thÆ° má»¥c gá»‘c
cd ..

# Cháº¡y training (máº¥t ~15 phÃºt CPU, ~3 phÃºt GPU)
python resolve.py

# Output mong Ä‘á»£i:
# ==================================================
# NHáº¬N Dáº NG CHá»® Sá» MNIST Vá»šI CNN  
# ==================================================
# Äang táº£i dá»¯ liá»‡u...
# KÃ­ch thÆ°á»›c dá»¯ liá»‡u:
# X_train: (33600, 28, 28, 1)
# X_val: (8400, 28, 28, 1)
# X_test: (28000, 28, 28, 1)
# ...
# Validation Accuracy: 0.9940
```

### 5. ðŸ“Š **Kiá»ƒm tra káº¿t quáº£**

Sau khi cháº¡y xong, sáº½ cÃ³ cÃ¡c files má»›i:

```bash
ls -la *.png *.csv
# training_history.png     - Biá»ƒu Ä‘á»“ training process
# confusion_matrix.png     - Ma tráº­n nháº§m láº«n  
# predictions_sample.png   - VÃ­ dá»¥ predictions
# submission.csv           - File káº¿t quáº£ cuá»‘i (28,000 dá»± Ä‘oÃ¡n)
```

### 6. ðŸ”§ **Troubleshooting Installation**

#### Lá»—i thÆ°á»ng gáº·p:

**1. TensorFlow installation failed:**
```bash
# Thá»­ cÃ i TensorFlow CPU version
pip install tensorflow-cpu>=2.8.0

# Hoáº·c dÃ¹ng conda
conda install tensorflow-gpu  # Náº¿u cÃ³ GPU
conda install tensorflow      # CPU only
```

**2. Memory Error:**
```bash
# Giáº£m batch size trong resolve.py
# TÃ¬m dÃ²ng: batch_size=128
# Thay thÃ nh: batch_size=64 hoáº·c batch_size=32
```

**3. Permission Error (Windows):**
```bash
# Cháº¡y PowerShell as Administrator
# Hoáº·c thay Ä‘Æ°á»ng dáº«n khÃ´ng cÃ³ space:
# Thay vÃ¬: "G:\My Drive\..." 
# DÃ¹ng: "C:\projects\buoi3_e1"
```

**4. CUDA Error (GPU):**
```bash
# Kiá»ƒm tra GPU
python -c "import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))"

# Náº¿u khÃ´ng cÃ³ GPU, TensorFlow tá»± chuyá»ƒn sang CPU
# Performance sáº½ cháº­m hÆ¡n nhÆ°ng váº«n cháº¡y Ä‘Æ°á»£c
```

## Cáº¥u trÃºc dá»± Ã¡n

```
buoi3_e1/
â”œâ”€â”€ dataset/                 # ThÆ° má»¥c chá»©a dá»¯ liá»‡u
â”‚   â”œâ”€â”€ train.csv           # Dá»¯ liá»‡u training (42,000 samples)
â”‚   â”œâ”€â”€ test.csv            # Dá»¯ liá»‡u test (28,000 samples)
â”‚   â””â”€â”€ sample_submission.csv
â”œâ”€â”€ resolve.py              # File code chÃ­nh
â”œâ”€â”€ requirements.txt        # Dependencies
â”œâ”€â”€ README.md              # File hÆ°á»›ng dáº«n nÃ y
â””â”€â”€ outputs/               # ThÆ° má»¥c káº¿t quáº£ (tá»± Ä‘á»™ng táº¡o)
    â”œâ”€â”€ submission.csv     # File submission
    â”œâ”€â”€ training_history.png
    â”œâ”€â”€ confusion_matrix.png
    â””â”€â”€ predictions_sample.png
```

## Káº¿t quáº£ thá»±c táº¿ Ä‘áº¡t Ä‘Æ°á»£c

### ðŸŽ‰ **Hiá»‡u suáº¥t xuáº¥t sáº¯c**
- **Validation Accuracy**: **99.40%** 
- **Training Accuracy**: **98.90%**
- **Training Time**: 41 epochs (vá»›i Early Stopping)
- **Total Training Time**: ~15 phÃºt trÃªn CPU

### ðŸ“Š **Classification Report chi tiáº¿t**
```
              precision    recall  f1-score   support
           0       1.00      1.00      1.00       827
           1       1.00      0.99      0.99       937
           2       0.99      1.00      0.99       835
           3       1.00      1.00      1.00       870
           4       0.99      0.99      0.99       814
           5       0.99      1.00      1.00       759
           6       1.00      0.99      1.00       827
           7       0.99      0.99      0.99       880
           8       0.99      1.00      0.99       813
           9       1.00      0.99      0.99       838

    accuracy                           0.99      8400
   macro avg       0.99      0.99      0.99      8400
weighted avg       0.99      0.99      0.99      8400
```

### ðŸ“ˆ **QuÃ¡ trÃ¬nh training**
- **Epoch 1**: 57.95% accuracy â†’ 41: 99.40% accuracy
- **Learning Rate Reduction**: 0.001 â†’ 0.0002 â†’ 0.0001 (tá»± Ä‘á»™ng)
- **Early Stopping**: Dá»«ng khi khÃ´ng cáº£i thiá»‡n trong 10 epochs
- **Best Performance**: Epoch 31 vá»›i 99.40% val_accuracy

## Káº¿t quáº£ mong Ä‘á»£i

- **Training Accuracy**: ~99%+ âœ…
- **Validation Accuracy**: ~98%+ âœ… (Äáº¡t 99.40%)
- **Test Performance**: Dá»± kiáº¿n Ä‘áº¡t accuracy cao trÃªn leaderboard

## Giáº£i thÃ­ch code chi tiáº¿t

### ðŸ—ï¸ **Cáº¥u trÃºc Class DigitRecognizer**

```python
class DigitRecognizer:
    def __init__(self):
        self.model = None      # LÆ°u trá»¯ CNN model
        self.history = None    # LÆ°u training history
```

#### 1. `load_data()` - Táº£i vÃ  xá»­ lÃ½ dá»¯ liá»‡u
```python
def load_data(self):
    # BÆ°á»›c 1: Äá»c CSV files
    train_df = pd.read_csv('dataset/train.csv')  # 42,000 samples
    test_df = pd.read_csv('dataset/test.csv')    # 28,000 samples
    
    # BÆ°á»›c 2: TÃ¡ch features vÃ  labels  
    X = train_df.drop('label', axis=1).values    # 784 pixel values
    y = train_df['label'].values                 # Digit labels (0-9)
    
    # BÆ°á»›c 3: Reshape thÃ nh áº£nh 28x28
    X = X.reshape(-1, 28, 28, 1)                # (N, 28, 28, 1)
    
    # BÆ°á»›c 4: Normalize vá» [0,1]
    X = X.astype('float32') / 255.0              # Tá»« [0,255] â†’ [0,1]
    
    # BÆ°á»›c 5: One-hot encoding cho labels
    y = keras.utils.to_categorical(y, 10)        # [3] â†’ [0,0,0,1,0,0,0,0,0,0]
    
    # BÆ°á»›c 6: Chia train/validation (80/20)
    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)
```

**Giáº£i thÃ­ch quan trá»ng:**
- **Reshape**: MNIST gá»‘c lÃ  vector 784 chiá»u â†’ CNN cáº§n input 2D (28x28)
- **Normalization**: GiÃºp model há»c nhanh hÆ¡n vÃ  á»•n Ä‘á»‹nh hÆ¡n
- **One-hot encoding**: CNN output 10 neurons â†’ cáº§n label dáº¡ng [0,0,1,0,...]

#### 2. `build_model()` - XÃ¢y dá»±ng kiáº¿n trÃºc CNN

```python
def build_model(self):
    model = keras.Sequential([
        # Data Augmentation (ngáº«u nhiÃªn hÃ³a áº£nh)
        layers.RandomRotation(0.1),        # Xoay Â±10%
        layers.RandomZoom(0.1),            # Zoom Â±10%
        layers.RandomTranslation(0.1, 0.1), # Dá»‹ch chuyá»ƒn Â±10%
        
        # Convolutional Block 1
        layers.Conv2D(32, (3,3), activation='relu'),  # 32 filters 3x3
        layers.BatchNormalization(),                   # Chuáº©n hÃ³a batch
        layers.Conv2D(32, (3,3), activation='relu'),  # ThÃªm 1 Conv layer
        layers.MaxPooling2D((2,2)),                   # Giáº£m size xuá»‘ng 1/2
        layers.Dropout(0.25),                         # Ngáº«u nhiÃªn táº¯t 25% neurons
        
        # Convolutional Block 2 (tÆ°Æ¡ng tá»± nhÆ°ng 64 filters)
        layers.Conv2D(64, (3,3), activation='relu'),
        layers.BatchNormalization(),
        layers.Conv2D(64, (3,3), activation='relu'),
        layers.MaxPooling2D((2,2)),
        layers.Dropout(0.25),
        
        # Convolutional Block 3 (128 filters)
        layers.Conv2D(128, (3,3), activation='relu'),
        layers.BatchNormalization(),
        layers.Dropout(0.25),
        
        # Chuyá»ƒn tá»« 2D sang 1D
        layers.Flatten(),
        
        # Dense Layers cho classification
        layers.Dense(512, activation='relu'),
        layers.BatchNormalization(),
        layers.Dropout(0.5),
        layers.Dense(256, activation='relu'), 
        layers.Dropout(0.5),
        layers.Dense(10, activation='softmax')  # 10 classes (0-9)
    ])
```

**Giáº£i thÃ­ch tá»«ng thÃ nh pháº§n:**

1. **Data Augmentation**: Táº¡o thÃªm dá»¯ liá»‡u báº±ng cÃ¡ch biáº¿n Ä‘á»•i áº£nh
2. **Conv2D**: PhÃ¡t hiá»‡n patterns (edges, shapes) vá»›i sliding window
3. **BatchNormalization**: Chuáº©n hÃ³a Ä‘á»ƒ training á»•n Ä‘á»‹nh hÆ¡n
4. **MaxPooling2D**: Giáº£m kÃ­ch thÆ°á»›c, giá»¯ láº¡i thÃ´ng tin quan trá»ng nháº¥t
5. **Dropout**: Ngáº«u nhiÃªn "táº¯t" neurons Ä‘á»ƒ trÃ¡nh overfitting
6. **Dense**: Fully connected layers cho classification cuá»‘i cÃ¹ng

#### 3. `train_model()` - Huáº¥n luyá»‡n vá»›i Callbacks

```python
def train_model(self, X_train, X_val, y_train, y_val, epochs=50):
    callbacks = [
        # Dá»«ng sá»›m náº¿u khÃ´ng cáº£i thiá»‡n
        keras.callbacks.EarlyStopping(
            monitor='val_accuracy',
            patience=10,                    # Chá» 10 epochs
            restore_best_weights=True       # Láº¥y láº¡i weights tá»‘t nháº¥t
        ),
        
        # Giáº£m learning rate khi loss plateau
        keras.callbacks.ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.2,                     # Giáº£m 5 láº§n (Ã—0.2)
            patience=5,                     # Chá» 5 epochs
            min_lr=0.0001                   # KhÃ´ng giáº£m dÆ°á»›i 0.0001
        )
    ]
```

**Callbacks giáº£i thÃ­ch:**
- **EarlyStopping**: Tá»± Ä‘á»™ng dá»«ng khi model khÃ´ng cáº£i thiá»‡n â†’ tiáº¿t kiá»‡m thá»i gian
- **ReduceLROnPlateau**: Giáº£m learning rate khi "máº¯c káº¹t" â†’ giÃºp model há»c tinh hÆ¡n

#### 4. `evaluate_model()` - ÄÃ¡nh giÃ¡ chi tiáº¿t

```python
def evaluate_model(self, X_val, y_val):
    # Dá»± Ä‘oÃ¡n
    y_pred = self.model.predict(X_val)
    y_pred_classes = np.argmax(y_pred, axis=1)  # [0.1,0.9,0.0,...] â†’ 1
    y_true_classes = np.argmax(y_val, axis=1)   # One-hot â†’ class index
    
    # TÃ­nh accuracy
    accuracy = accuracy_score(y_true_classes, y_pred_classes)
    
    # Classification report (precision, recall, f1-score)
    print(classification_report(y_true_classes, y_pred_classes))
    
    # Confusion Matrix - xem model nháº§m láº«n á»Ÿ Ä‘Ã¢u
    cm = confusion_matrix(y_true_classes, y_pred_classes)
```

#### 5. Flow tá»•ng thá»ƒ cá»§a chÆ°Æ¡ng trÃ¬nh

```python
def main():
    # 1. Khá»Ÿi táº¡o
    recognizer = DigitRecognizer()
    
    # 2. Load vÃ  preprocess data
    X_train, X_val, y_train, y_val, X_test = recognizer.load_data()
    
    # 3. Build CNN model  
    model = recognizer.build_model()
    
    # 4. Train vá»›i validation
    history = recognizer.train_model(X_train, X_val, y_train, y_val)
    
    # 5. Evaluate vÃ  visualize
    accuracy = recognizer.evaluate_model(X_val, y_val)
    recognizer.plot_training_history()
    
    # 6. Predict trÃªn test set
    submission = recognizer.predict_and_save(X_test)
```

## Hyperparameters

| Parameter | Value | Giáº£i thÃ­ch |
|-----------|-------|------------|
| Learning Rate | Adam default (0.001) | Tá»± Ä‘á»™ng Ä‘iá»u chá»‰nh |
| Batch Size | 128 | CÃ¢n báº±ng tá»‘c Ä‘á»™ vÃ  memory |
| Epochs | 50 | Vá»›i early stopping |
| Dropout | 0.25-0.5 | TÄƒng dáº§n qua cÃ¡c layer |
| Data Aug Rate | 0.1 | Augmentation nháº¹ |

## Má»Ÿ rá»™ng vÃ  cáº£i tiáº¿n

### ÄÃ£ implement:
- [x] CNN cÆ¡ báº£n vá»›i Conv-ReLU-Pool
- [x] BatchNormalization  
- [x] Data augmentation nháº¹
- [x] Dropout tuning
- [x] Learning rate scheduling

### CÃ³ thá»ƒ thÃªm:
- [ ] **Cutout**: Random mask patches
- [ ] **Mixup**: Trá»™n áº£nh vÃ  labels
- [ ] **TTA (Test Time Augmentation)**: Augment khi predict
- [ ] **Ensemble**: Káº¿t há»£p nhiá»u models
- [ ] **Advanced architectures**: ResNet, DenseNet

## TÃ¡c giáº£

- **TÃªn**: Nguyá»…n Trung Kiá»‡t
- **Email**: [your-email@example.com]
- **GitHub**: [nguyentrungkiet](https://github.com/nguyentrungkiet)

## PhiÃªn báº£n

- **v1.0**: Implementation cÆ¡ báº£n vá»›i CNN baseline
- **Framework**: TensorFlow/Keras 2.8+
- **Python**: 3.8+

## License

Dá»± Ã¡n nÃ y Ä‘Æ°á»£c phÃ¡t triá»ƒn cho má»¥c Ä‘Ã­ch há»c táº­p trong Olympic AI.

---

### LÆ°u Ã½ quan trá»ng

1. **Thá»i gian training**: Khoáº£ng 10-30 phÃºt tÃ¹y GPU
2. **Memory requirement**: Tá»‘i thiá»ƒu 4GB RAM
3. **GPU support**: Khuyáº¿n khÃ­ch sá»­ dá»¥ng GPU Ä‘á»ƒ tÄƒng tá»‘c
4. **Results**: Káº¿t quáº£ cÃ³ thá»ƒ khÃ¡c nhau do random initialization

### ðŸ”§ **Troubleshooting thá»±c táº¿**

#### âŒ **Lá»—i thÆ°á»ng gáº·p vÃ  cÃ¡ch fix:**

**1. `Out of Memory` Error:**
```python
# SOLUTION: Giáº£m batch_size trong resolve.py
# TÃ¬m dÃ²ng 152:
batch_size=128,

# Thay thÃ nh:  
batch_size=64,    # Hoáº·c 32 náº¿u váº«n lá»—i
```

**2. `Slow Training` (>30 phÃºt):**
```bash
# CHECK: GPU cÃ³ Ä‘Æ°á»£c sá»­ dá»¥ng khÃ´ng?
python -c "import tensorflow as tf; print(f'GPU Available: {tf.config.list_physical_devices(\"GPU\")}')"

# SOLUTION: Náº¿u khÃ´ng cÃ³ GPU
# - Giáº£m epochs tá»« 50 â†’ 20
# - Giáº£m model size (Ã­t Conv layers)
```

**3. `Poor Accuracy` (<95%):**
```python
# POSSIBLE CAUSES & SOLUTIONS:
# 1. Data khÃ´ng Ä‘Ãºng format â†’ Kiá»ƒm tra load_data()
# 2. Learning rate quÃ¡ cao â†’ Thá»­ lr=0.0001  
# 3. Overfitting â†’ TÄƒng dropout lÃªn 0.5
# 4. Underfitting â†’ TÄƒng model capacity hoáº·c epochs
```

**4. `Module not found` Error:**
```bash
# SOLUTION: Reinstall dependencies
pip uninstall tensorflow pandas numpy matplotlib
pip install -r requirements.txt

# Hoáº·c dÃ¹ng conda:
conda install tensorflow pandas numpy matplotlib seaborn scikit-learn
```

#### ðŸ’¡ **Tips Ä‘á»ƒ cáº£i thiá»‡n káº¿t quáº£:**

**1. Hyperparameter Tuning:**
```python
# Thá»­ cÃ¡c giÃ¡ trá»‹ khÃ¡c nhau:
- learning_rate: [0.001, 0.0005, 0.0001]
- dropout_rate: [0.2, 0.3, 0.4, 0.5]  
- batch_size: [32, 64, 128, 256]
- data_augmentation: TÄƒng/giáº£m rotation, zoom
```

**2. Model Architecture:**
```python
# ThÃªm layers:
layers.Conv2D(256, (3,3), activation='relu'),  # Block 4
layers.GlobalAveragePooling2D(),               # Thay Flatten

# Hoáº·c thá»­ Transfer Learning:
base_model = tf.keras.applications.MobileNetV2(...)
```

**3. Advanced Techniques:**
```python
# Cutout (Random Erasing)
def cutout(image, mask_size=8):
    h, w = image.shape[:2]
    y = np.random.randint(h)
    x = np.random.randint(w)
    image[y:y+mask_size, x:x+mask_size] = 0

# Mixup  
def mixup(x1, x2, y1, y2, alpha=0.2):
    lam = np.random.beta(alpha, alpha)
    x = lam * x1 + (1 - lam) * x2
    y = lam * y1 + (1 - lam) * y2
    return x, y
```

#### ðŸ“ˆ **Monitoring Training:**

```python
# ThÃªm vÃ o train_model():
# ThÃªm TensorBoard callback
keras.callbacks.TensorBoard(
    log_dir='logs',
    histogram_freq=1,
    write_graph=True,
    write_images=True
)

# Cháº¡y TensorBoard Ä‘á»ƒ theo dÃµi:
# tensorboard --logdir=logs
```

#### ðŸŽ¯ **Benchmarks Ä‘á»ƒ so sÃ¡nh:**

| Method | Validation Accuracy | Training Time |
|--------|-------------------|---------------|
| **Our CNN** | **99.40%** | **15 min (CPU)** |
| Simple MLP | ~97.5% | 5 min |
| Basic CNN | ~98.5% | 10 min |
| ResNet-50 | ~99.6% | 30 min |
| Ensemble (5 models) | ~99.7% | 75 min |

â†’ **Káº¿t luáº­n**: Model cá»§a chÃºng ta Ä‘áº¡t balance tá»‘t giá»¯a accuracy vÃ  training time!

ChÃºc báº¡n thÃ nh cÃ´ng vá»›i dá»± Ã¡n! ðŸš€