# Nháº­n Dáº¡ng Chá»¯ Sá»‘ MNIST vá»›i CNN

## MÃ´ táº£ dá»± Ã¡n

ÄÃ¢y lÃ  dá»± Ã¡n nháº­n dáº¡ng chá»¯ sá»‘ viáº¿t tay tá»« dataset MNIST sá»­ dá»¥ng Convolutional Neural Network (CNN). Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn trong khuÃ´n khá»• Olympic AI - Buá»•i 3 - BÃ i táº­p E1.

## Má»¥c tiÃªu

- **Má»¥c tiÃªu chÃ­nh**: LÃ m quen vá»›i CNN cÆ¡ báº£n
- **Dataset**: Digit Recognizer (MNIST) 
- **Metric Ä‘Ã¡nh giÃ¡**: Accuracy
- **Kiáº¿n trÃºc**: CNN vá»›i cÃ¡c layer Convâ€“ReLUâ€“Pool, BatchNorm, data augmentation

## Äáº·c Ä‘iá»ƒm ká»¹ thuáº­t

### Kiáº¿n trÃºc mÃ´ hÃ¬nh

MÃ´ hÃ¬nh CNN Ä‘Æ°á»£c thiáº¿t káº¿ vá»›i:

1. **Data Augmentation Layer**:
   - RandomRotation (10%)
   - RandomZoom (10%) 
   - RandomTranslation (10%)

2. **3 Convolutional Blocks**:
   - **Block 1**: 2x Conv2D(32) + BatchNorm + MaxPool + Dropout(0.25)
   - **Block 2**: 2x Conv2D(64) + BatchNorm + MaxPool + Dropout(0.25)
   - **Block 3**: Conv2D(128) + BatchNorm + Dropout(0.25)

3. **Dense Layers**:
   - Flatten
   - Dense(512) + BatchNorm + Dropout(0.5)
   - Dense(256) + Dropout(0.5) 
   - Dense(10, softmax) - output layer

### Ká»¹ thuáº­t nÃ¢ng cao Ä‘Æ°á»£c Ã¡p dá»¥ng

- âœ… **BatchNormalization**: Cáº£i thiá»‡n tá»‘c Ä‘á»™ há»™i tá»¥ vÃ  á»•n Ä‘á»‹nh training
- âœ… **Data Augmentation**: TÄƒng cÆ°á»ng dá»¯ liá»‡u Ä‘á»ƒ trÃ¡nh overfitting
- âœ… **Dropout**: Regularization vá»›i tá»· lá»‡ 0.25-0.5
- âœ… **Early Stopping**: Dá»«ng training khi validation accuracy khÃ´ng cáº£i thiá»‡n
- âœ… **Learning Rate Scheduling**: Giáº£m learning rate khi loss khÃ´ng giáº£m

## CÃ i Ä‘áº·t vÃ  cháº¡y

### 1. CÃ i Ä‘áº·t mÃ´i trÆ°á»ng

```bash
# Clone repository
git clone https://github.com/nguyentrungkiet/buoi3_e1.git
cd buoi3_e1

# CÃ i Ä‘áº·t dependencies
pip install -r requirements.txt
```

### 2. Chuáº©n bá»‹ dá»¯ liá»‡u

Äáº£m báº£o cÃ¡c file dá»¯ liá»‡u Ä‘Æ°á»£c Ä‘áº·t trong thÆ° má»¥c `dataset/`:
```
dataset/
â”œâ”€â”€ train.csv
â”œâ”€â”€ test.csv
â””â”€â”€ sample_submission.csv
```

### 3. Cháº¡y training

```bash
python resolve.py
```

## Cáº¥u trÃºc dá»± Ã¡n

```
buoi3_e1/
â”œâ”€â”€ dataset/                 # ThÆ° má»¥c chá»©a dá»¯ liá»‡u
â”‚   â”œâ”€â”€ train.csv           # Dá»¯ liá»‡u training (42,000 samples)
â”‚   â”œâ”€â”€ test.csv            # Dá»¯ liá»‡u test (28,000 samples)
â”‚   â””â”€â”€ sample_submission.csv
â”œâ”€â”€ resolve.py              # File code chÃ­nh
â”œâ”€â”€ requirements.txt        # Dependencies
â”œâ”€â”€ README.md              # File hÆ°á»›ng dáº«n nÃ y
â””â”€â”€ outputs/               # ThÆ° má»¥c káº¿t quáº£ (tá»± Ä‘á»™ng táº¡o)
    â”œâ”€â”€ submission.csv     # File submission
    â”œâ”€â”€ training_history.png
    â”œâ”€â”€ confusion_matrix.png
    â””â”€â”€ predictions_sample.png
```

## Káº¿t quáº£ mong Ä‘á»£i

- **Training Accuracy**: ~99%+
- **Validation Accuracy**: ~98%+
- **Test Performance**: Dá»± kiáº¿n Ä‘áº¡t accuracy cao trÃªn leaderboard

## Giáº£i thÃ­ch code

### Class DigitRecognizer

ÄÃ¢y lÃ  class chÃ­nh chá»©a toÃ n bá»™ logic:

#### 1. `load_data()` 
- Táº£i vÃ  tiá»n xá»­ lÃ½ dá»¯ liá»‡u
- Reshape áº£nh vá» 28x28x1
- Normalize pixel values vá» [0,1]
- Chia train/validation set

#### 2. `build_model()`
- XÃ¢y dá»±ng kiáº¿n trÃºc CNN
- Ãp dá»¥ng data augmentation
- Compile model vá»›i Adam optimizer

#### 3. `train_model()`
- Huáº¥n luyá»‡n mÃ´ hÃ¬nh vá»›i callbacks
- Early stopping vÃ  learning rate reduction
- Validation trÃªn 20% dá»¯ liá»‡u train

#### 4. `evaluate_model()`
- ÄÃ¡nh giÃ¡ performance
- Táº¡o confusion matrix
- Classification report chi tiáº¿t

#### 5. `predict_and_save()`
- Dá»± Ä‘oÃ¡n trÃªn test set
- LÆ°u káº¿t quáº£ theo format yÃªu cáº§u

### TÃ­nh nÄƒng ná»•i báº­t

1. **Reproducibility**: Set random seeds cho káº¿t quáº£ nháº¥t quÃ¡n
2. **Visualization**: Tá»± Ä‘á»™ng táº¡o cÃ¡c biá»ƒu Ä‘á»“ phÃ¢n tÃ­ch
3. **Modular Design**: Code Ä‘Æ°á»£c tá»• chá»©c thÃ nh class dá»… hiá»ƒu
4. **Comprehensive Evaluation**: ÄÃ¡nh giÃ¡ Ä‘a chiá»u vá»›i nhiá»u metrics

## Hyperparameters

| Parameter | Value | Giáº£i thÃ­ch |
|-----------|-------|------------|
| Learning Rate | Adam default (0.001) | Tá»± Ä‘á»™ng Ä‘iá»u chá»‰nh |
| Batch Size | 128 | CÃ¢n báº±ng tá»‘c Ä‘á»™ vÃ  memory |
| Epochs | 50 | Vá»›i early stopping |
| Dropout | 0.25-0.5 | TÄƒng dáº§n qua cÃ¡c layer |
| Data Aug Rate | 0.1 | Augmentation nháº¹ |

## Má»Ÿ rá»™ng vÃ  cáº£i tiáº¿n

### ÄÃ£ implement:
- [x] CNN cÆ¡ báº£n vá»›i Conv-ReLU-Pool
- [x] BatchNormalization  
- [x] Data augmentation nháº¹
- [x] Dropout tuning
- [x] Learning rate scheduling

### CÃ³ thá»ƒ thÃªm:
- [ ] **Cutout**: Random mask patches
- [ ] **Mixup**: Trá»™n áº£nh vÃ  labels
- [ ] **TTA (Test Time Augmentation)**: Augment khi predict
- [ ] **Ensemble**: Káº¿t há»£p nhiá»u models
- [ ] **Advanced architectures**: ResNet, DenseNet

## TÃ¡c giáº£

- **TÃªn**: Nguyá»…n Trung Kiá»‡t
- **Email**: [your-email@example.com]
- **GitHub**: [nguyentrungkiet](https://github.com/nguyentrungkiet)

## PhiÃªn báº£n

- **v1.0**: Implementation cÆ¡ báº£n vá»›i CNN baseline
- **Framework**: TensorFlow/Keras 2.8+
- **Python**: 3.8+

## License

Dá»± Ã¡n nÃ y Ä‘Æ°á»£c phÃ¡t triá»ƒn cho má»¥c Ä‘Ã­ch há»c táº­p trong Olympic AI.

---

### LÆ°u Ã½ quan trá»ng

1. **Thá»i gian training**: Khoáº£ng 10-30 phÃºt tÃ¹y GPU
2. **Memory requirement**: Tá»‘i thiá»ƒu 4GB RAM
3. **GPU support**: Khuyáº¿n khÃ­ch sá»­ dá»¥ng GPU Ä‘á»ƒ tÄƒng tá»‘c
4. **Results**: Káº¿t quáº£ cÃ³ thá»ƒ khÃ¡c nhau do random initialization

### Troubleshooting

**Lá»—i thÆ°á»ng gáº·p:**

1. **Out of Memory**: Giáº£m batch_size xuá»‘ng 64 hoáº·c 32
2. **Slow training**: Kiá»ƒm tra GPU setup
3. **Poor accuracy**: TÄƒng epochs hoáº·c Ä‘iá»u chá»‰nh learning rate

**Tips Ä‘á»ƒ cáº£i thiá»‡n:**

1. Thá»­ nghiá»‡m vá»›i different dropout rates
2. Adjust data augmentation parameters  
3. Experiment vá»›i model architecture
4. Use different optimizers (SGD, RMSprop)

ChÃºc báº¡n thÃ nh cÃ´ng vá»›i dá»± Ã¡n! ğŸš€